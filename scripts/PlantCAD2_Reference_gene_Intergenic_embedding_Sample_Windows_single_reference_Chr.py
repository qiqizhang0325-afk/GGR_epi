"""
å®Œæ•´çš„å…¨åŸºå› ç»„ embedding è„šæœ¬ï¼ˆreference + per-sample mutantï¼‰
- åŸºå› åŒº + åŸºå› é—´åŒº
- PlantCAD2: single-nucleotide tokenization
- 8192bp window + 4096bp stride
- Attention pooling èåˆçª—å£
- å˜å¼‚æ³¨å…¥ï¼šä»… SNPï¼ˆåŸºäº VCFï¼Œ600+ æ ·æœ¬ï¼‰
"""

import gc
import os
import time

import pickle
import warnings
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Union
from datetime import datetime
import argparse

from Bio import SeqIO
from Bio.Seq import Seq
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForMaskedLM
from tqdm import tqdm
from cyvcf2 import VCF

from bisect import bisect_left
from collections import defaultdict


# å‡è®¾ DataValidator ç±»å·²ç»å®šä¹‰


warnings.filterwarnings('ignore')


def ensure_dir(path: Union[str, Path]) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


# =====================
#   GFF åŸºå› æ•°æ®åº“
# =====================


# vcf_file='/lustre/BIF/nobackup/zhang479/genomes/arabidopsis/test.vcf'
# gff_file='/lustre/BIF/nobackup/zhang479/genomes/arabidopsis/TAIR10_GFF3_genes.gff'
# fasta_file='/lustre/BIF/nobackup/zhang479/genomes/arabidopsis/TAIR10_chr_all.fas'


class GeneDatabase:
    """åŸºå› ä¿¡æ¯æ•°æ®åº“ - ä» GFF æ–‡ä»¶æ„å»º"""

    def __init__(self, gff_file: str):
        self.genes = self._parse_gff(gff_file)

    def _parse_gff(self, gff_file: str) -> dict:
        print(f"ğŸ“ è§£æ GFF æ–‡ä»¶: {gff_file}")
        genes = {}

        try:
            with open(gff_file, 'r') as f:
                for line in f:
                    if line.startswith('#'):
                        continue

                    parts = line.strip().split('\t')
                    if len(parts) < 9:
                        continue

                    chrom = parts[0]
                    feature = parts[2]
                    start = int(parts[3])      # 1-based
                    end = int(parts[4])        # inclusive
                    strand = parts[6]
                    attributes = parts[8]

                    # ç®€å•è§„èŒƒï¼šçº¯æ•°å­—å‰é¢è¡¥ Chr
                    if not chrom.startswith('Chr') and chrom.isdigit():
                        chrom = f'Chr{chrom}'

                    if feature != 'gene':
                        continue

                    gene_id = self._extract_gene_id(attributes)
                    if gene_id:
                        genes[gene_id] = {
                            'chrom': chrom,
                            'start': start,
                            'end': end,
                            'strand': strand
                        }
        except Exception as e:
            print(f"âŒ GFF æ–‡ä»¶è§£æé”™è¯¯: {e}")
            raise

        print(f"âœ… æ‰¾åˆ° {len(genes)} ä¸ªåŸºå› ")
        return genes

    def _extract_gene_id(self, attributes: str) -> Optional[str]:
        for attr in attributes.split(';'):
            attr = attr.strip()
            if attr.startswith('ID='):
                return attr.split('ID=')[1].split(',')[0]
            elif attr.startswith('gene='):
                return attr.split('gene=')[1].split(',')[0]
        return None

    def get_gene_info(self, gene_id: str) -> Optional[dict]:
        return self.genes.get(gene_id)


# =====================
#   è¾“å…¥éªŒè¯ & æŸ“è‰²ä½“æ ‡å‡†åŒ–
# =====================

'''
class DataValidator:
    @staticmethod
    def ensure_bgzip_and_index(vcf_file: str) -> str:
        vcf = Path(vcf_file)
        if vcf.suffix == ".gz":
            tbi = str(vcf) + ".tbi"
            if not os.path.exists(tbi):
                print(f"ğŸ”§ ç¼ºå¤±ç´¢å¼•ï¼Œæ­£åœ¨åˆ›å»º: {tbi}")
                subprocess.run(["tabix", "-p", "vcf", str(vcf)], check=True)
            return str(vcf)

        gz = str(vcf) + ".gz"
        tbi = gz + ".tbi"
        if not os.path.exists(gz):
            print(f"ğŸ”§ bgzip å‹ç¼© VCF â†’ {gz}")
            with open(gz, "wb") as out_f:
                subprocess.run(["bgzip", "-c", str(vcf)], stdout=out_f, check=True)
        if not os.path.exists(tbi):
            print(f"ğŸ”§ åˆ›å»ºç´¢å¼•: {tbi}")
            subprocess.run(["tabix", "-p", "vcf", gz], check=True)
        return gz

    @staticmethod
    def validate_inputs(fasta_file: str, vcf_file: str, gff_file: str) -> bool:
        print("\nğŸ” éªŒè¯è¾“å…¥æ–‡ä»¶...")
        all_valid = True
        for file_path, file_type in [(fasta_file, "FASTA"),
                                     (vcf_file, "VCF"),
                                     (gff_file, "GFF")]:
            if not Path(file_path).exists():
                print(f"âŒ {file_type} æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                all_valid = False
            else:
                size_gb = Path(file_path).stat().st_size / (1024**3)
                print(f"âœ… {file_type}: {size_gb:.2f} GB")
        return all_valid

    @staticmethod
    def standardize_chrom_name(chrom: str) -> str:
        return chrom.replace("Chr", "").replace("chr", "").upper()

    @staticmethod
    def normalize_chromosome_names(genome_keys: List[str],
                                   vcf_chroms: List[str],
                                   gff_chroms: List[str]) -> Dict[str, Dict[str, str]]:
        print("\nğŸ”§ æ ‡å‡†åŒ–æŸ“è‰²ä½“åç§°...")

        genome_map = {DataValidator.standardize_chrom_name(c): c for c in genome_keys}
        vcf_map = {DataValidator.standardize_chrom_name(c): c for c in vcf_chroms}
        gff_map = {DataValidator.standardize_chrom_name(c): c for c in gff_chroms}

        all_keys = set(genome_map) | set(vcf_map) | set(gff_map)

        normalized_mapping = {}
        for k in sorted(all_keys):
            normalized_mapping[k] = {
                'fasta': genome_map.get(k),
                'vcf': vcf_map.get(k),
                'gff': gff_map.get(k)
            }

        print(f"âœ… æ ‡å‡†åŒ–å®Œæˆï¼Œæ£€æµ‹åˆ° {len(normalized_mapping)} æ¡æŸ“è‰²ä½“æ˜ å°„")
        for ek in list(normalized_mapping.keys())[:5]:
            print(f"   {ek}: {normalized_mapping[ek]}")

        return normalized_mapping
'''



class DataValidator:
    @staticmethod
    def ensure_bgzip_and_index(vcf_file: str) -> str:
        """
        ç¡®ä¿ VCF æ–‡ä»¶æ˜¯ BGZIP å‹ç¼©ä¸”æ‹¥æœ‰ tabix ç´¢å¼•ã€‚
        å¦‚æœæ–‡ä»¶æ˜¯æ™®é€š GZIP å‹ç¼©ï¼Œåˆ™å…ˆè§£å‹å†ç”¨ bgzip é‡æ–°å‹ç¼©ã€‚
        è¿”å›æœ€ç»ˆçš„ BGZIP æ–‡ä»¶è·¯å¾„ã€‚
        """
        vcf_path = Path(vcf_file)
        
        # 1. å¦‚æœæ–‡ä»¶æ˜¯ .gz æ ¼å¼
        if vcf_path.suffix == ".gz":
            # å‡è®¾æ–‡ä»¶å·²ç»æ˜¯ BGZIPï¼Œå…ˆå°è¯•ç›´æ¥ç´¢å¼•
            print(f"ğŸ”„ æ£€æŸ¥å‹ç¼© VCF æ–‡ä»¶: {vcf_path.name}")
            gz_file = str(vcf_path)
            tbi_file = gz_file + ".tbi"

            if os.path.exists(tbi_file):
                print(f"âœ… ç´¢å¼•å·²å­˜åœ¨: {tbi_file}")
                return gz_file

            print(f"ğŸ”§ å°è¯•åˆ›å»ºç´¢å¼•: {tbi_file}")
            
            try:
                # å°è¯•å¯¹ç°æœ‰ .gz æ–‡ä»¶åˆ›å»ºç´¢å¼•
                subprocess.run(
                    ["tabix", "-p", "vcf", gz_file], 
                    check=True,  # å¦‚æœ tabix å¤±è´¥ï¼Œå°†æŠ›å‡º CalledProcessError
                    stdout=subprocess.PIPE, 
                    stderr=subprocess.PIPE
                )
                print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸï¼Œæ–‡ä»¶ä¸º BGZIP æ ¼å¼ã€‚")
                return gz_file
            
            except subprocess.CalledProcessError as e:
                # å¦‚æœ tabix å¤±è´¥ï¼Œé€šå¸¸æ„å‘³ç€å®ƒä¸æ˜¯ BGZF æ–‡ä»¶
                print(f"âš ï¸ tabix å¤±è´¥ï¼Œæ–‡ä»¶å¯èƒ½ä¸æ˜¯ BGZIP æ ¼å¼ã€‚é”™è¯¯ä¿¡æ¯ï¼ˆéƒ¨åˆ†ï¼‰ï¼š{e.stderr.decode().strip()}")
                
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºè§£å‹æ“ä½œ
                uncompressed_vcf = vcf_path.with_suffix('')
                
                # --- A. è§£å‹æ–‡ä»¶ ---
                print(f"ğŸ”§ å‡å®šä¸ºæ™®é€š GZIPï¼Œæ­£åœ¨è§£å‹ â†’ {uncompressed_vcf.name}")
                try:
                    # ä½¿ç”¨ gzip -d (æˆ–ç›´æ¥è°ƒç”¨ gunzip) è¿›è¡Œè§£å‹
                    subprocess.run(["gzip", "-d", "-f", gz_file], check=True)
                except subprocess.CalledProcessError:
                    print("âŒ è§£å‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™æˆ–æ ¼å¼ã€‚")
                    raise

                # --- B. é‡æ–°æ‰§è¡Œ BGZIP å‹ç¼©å’Œç´¢å¼•æµç¨‹ ---
                # æ­¤æ—¶ uncompressed_vcf åº”è¯¥å­˜åœ¨
                vcf_path = uncompressed_vcf


        # 2. æ–‡ä»¶æ˜¯æœªå‹ç¼©çš„ VCF æ–‡ä»¶ (æˆ–åˆšä» GZIP ä¸­è§£å‹å‡ºæ¥)
        
        # ç›®æ ‡ BGZIP æ–‡ä»¶å
        gz_file = str(vcf_path) + ".gz"
        tbi_file = gz_file + ".tbi"

        # --- C. å‹ç¼©æ–‡ä»¶ ---
        needs_compression = not os.path.exists(gz_file) or (os.path.exists(gz_file) and os.path.exists(vcf_path))

        if needs_compression:
            if os.path.exists(gz_file):
                print(f"âš ï¸ å‘ç°å·²å­˜åœ¨çš„ {Path(gz_file).name}ï¼Œä½†åŸå§‹VCFæ–‡ä»¶ä»å­˜åœ¨ï¼Œé‡æ–°æ‰§è¡Œ BGZIP å‹ç¼©ã€‚")
                os.remove(gz_file) # åˆ é™¤æ—§çš„ï¼ˆå¯èƒ½æ˜¯æ™®é€š GZIP æˆ–ä¸å®Œæ•´çš„ï¼‰å‹ç¼©æ–‡ä»¶
                # å¦‚æœç´¢å¼•å­˜åœ¨ï¼Œä¹Ÿåˆ é™¤ï¼Œç¡®ä¿é‡æ–°ç´¢å¼•
                if os.path.exists(tbi_file):
                    os.remove(tbi_file)
            
            print(f"ğŸ”§ BGZIP å‹ç¼© VCF â†’ {Path(gz_file).name}")
            try:
                # ä½¿ç”¨ bgzip -c å‹ç¼©å¹¶é‡å®šå‘è¾“å‡ºåˆ°æ–°æ–‡ä»¶
                with open(gz_file, "wb") as out_f:
                    subprocess.run(["bgzip", "-c", str(vcf_path)], stdout=out_f, check=True)
                # å‹ç¼©æˆåŠŸåï¼Œåˆ é™¤åŸå§‹æœªå‹ç¼©æ–‡ä»¶
                os.remove(vcf_path) 
            except subprocess.CalledProcessError:
                print("âŒ BGZIP å‹ç¼©å¤±è´¥ï¼Œè¯·æ£€æŸ¥ bgzip æ˜¯å¦å¯ç”¨ã€‚")
                raise
        else:
            print(f"âœ… BGZIP æ–‡ä»¶å·²å­˜åœ¨: {Path(gz_file).name}")


        # --- D. åˆ›å»ºç´¢å¼• (ç°åœ¨æˆ‘ä»¬ç¡®å®š gz_file åº”è¯¥æ˜¯ä¸€ä¸ª BGZIP æ–‡ä»¶) ---
        if not os.path.exists(tbi_file):
            print(f"ğŸ”§ åˆ›å»ºç´¢å¼•: {Path(tbi_file).name}")
            try:
                # å¯¹æ–°åˆ›å»ºçš„ BGZIP æ–‡ä»¶åˆ›å»ºç´¢å¼•
                subprocess.run(["tabix", "-p", "vcf", gz_file], check=True)
                print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸã€‚")
            except subprocess.CalledProcessError as e:
                # å†æ¬¡å¤±è´¥ï¼Œè¿™æ‰æ˜¯çœŸæ­£çš„é—®é¢˜ï¼Œå¯èƒ½æ˜¯ tabix/vcf æ–‡ä»¶å†…å®¹é—®é¢˜
                print("âŒ Tabix ç´¢å¼•åˆ›å»ºå¤±è´¥ã€‚")
                # é’ˆå¯¹æ‚¨çš„æƒ…å†µï¼šå¦‚æœ Tabix å¤±è´¥ï¼Œè¿™å¯èƒ½ä»ç„¶æ˜¯æ–‡ä»¶æ ¼å¼é—®é¢˜ï¼Œ
                # ä½†æ—¢ç„¶æˆ‘ä»¬åˆšåˆšç”¨ bgzip å‹ç¼©äº†ï¼Œæ›´å¯èƒ½æ˜¯ VCF æ ¼å¼æœ¬èº«çš„é—®é¢˜ï¼ˆæ¯”å¦‚æœªæ’åºï¼‰ã€‚
                print(f"è‡´å‘½é”™è¯¯ï¼šè™½ç„¶åˆšåˆšæ‰§è¡Œäº† BGZIP å‹ç¼©ï¼Œä½† Tabix ä»å¤±è´¥ã€‚è¯·æ£€æŸ¥ VCF æ–‡ä»¶æ˜¯å¦å·²æ’åºã€‚")
                # æ­¤æ—¶ï¼Œåº”è¯¥è®©ç”¨æˆ·çœ‹åˆ°åŸå§‹çš„ tabix é”™è¯¯ä¿¡æ¯
                if e.stderr:
                    print(f"Tabix é”™è¯¯è¾“å‡º: {e.stderr.decode().strip()}")
                raise
        else:
            print(f"âœ… ç´¢å¼•æ–‡ä»¶å·²å­˜åœ¨: {Path(tbi_file).name}")
            
        return gz_file

    @staticmethod
    def validate_inputs(fasta_file: str, vcf_file: str, gff_file: str) -> bool:
        print("\nğŸ” éªŒè¯è¾“å…¥æ–‡ä»¶...")
        all_valid = True
        for file_path, file_type in [(fasta_file, "FASTA"),
                                     (vcf_file, "VCF"),
                                     (gff_file, "GFF")]:
            if not Path(file_path).exists():
                print(f"âŒ {file_type} æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                all_valid = False
            else:
                size_gb = Path(file_path).stat().st_size / (1024**3)
                print(f"âœ… {file_type}: {size_gb:.2f} GB")
        return all_valid

    @staticmethod
    def standardize_chrom_name(chrom: str) -> str:
        return chrom.replace("Chr", "").replace("chr", "").upper()

    @staticmethod
    def normalize_chromosome_names(genome_keys: List[str],
                                   vcf_chroms: List[str],
                                   gff_chroms: List[str]) -> Dict[str, Dict[str, str]]:
        print("\nğŸ”§ æ ‡å‡†åŒ–æŸ“è‰²ä½“åç§°...")

        genome_map = {DataValidator.standardize_chrom_name(c): c for c in genome_keys}
        vcf_map = {DataValidator.standardize_chrom_name(c): c for c in vcf_chroms}
        gff_map = {DataValidator.standardize_chrom_name(c): c for c in gff_chroms}

        all_keys = set(genome_map) | set(vcf_map) | set(gff_map)

        normalized_mapping = {}
        for k in sorted(all_keys):
            normalized_mapping[k] = {
                'fasta': genome_map.get(k),
                'vcf': vcf_map.get(k),
                'gff': gff_map.get(k)
            }

        print(f"âœ… æ ‡å‡†åŒ–å®Œæˆï¼Œæ£€æµ‹åˆ° {len(normalized_mapping)} æ¡æŸ“è‰²ä½“æ˜ å°„")
        for ek in list(normalized_mapping.keys())[:5]:
            print(f"   {ek}: {normalized_mapping[ek]}")

        return normalized_mapping



# =====================
#   VCF åŒ…è£…
# =====================

class PopulationVCF:
    """
    ç¾¤ä½“ VCF å°è£…
    - åªå¤„ç† SNP
    - æä¾›æ ·æœ¬åˆ—è¡¨ + cyvcf2.VCF å¯¹è±¡ + sample_index
    """

    def __init__(self, vcf_file: str):
        print(f"\nğŸ“¥ åŠ è½½ VCF: {vcf_file}")
        self.vcf = VCF(vcf_file)
        self.samples = self.vcf.samples
        self.sample_index = {s: i for i, s in enumerate(self.samples)}
        print(f"ğŸ“Š VCF åŒ…å« {len(self.samples)} ä¸ªæ ·æœ¬")

    def get_chroms(self) -> List[str]:
        return list(self.vcf.seqnames)


# =====================
#   åºåˆ—è´¨é‡æ£€æŸ¥
# =====================

class SequenceQualityChecker:
    @staticmethod
    def validate_dna_sequence(sequence: str) -> tuple:
        sequence = sequence.upper()

        valid_bases = set('ATCGN')
        invalid_bases = set(sequence) - valid_bases
        if invalid_bases:
            print(f"âš ï¸  å‘ç°æ— æ•ˆç¢±åŸº {invalid_bases}")
            for base in invalid_bases:
                sequence = sequence.replace(base, 'N')

        if len(sequence) < 50:
            print(f"âš ï¸  åºåˆ—è¿‡çŸ­ ({len(sequence)} bp)")

        if len(sequence) > 8192:
            print(f"â„¹ï¸  åºåˆ—è¿‡é•¿ ({len(sequence)} bp)ï¼Œå°†è¢«æˆªæ–­åˆ°8192bp")
            sequence = sequence[:8192]

        gc_content = (sequence.count('G') + sequence.count('C')) / len(sequence) * 100

        return sequence, gc_content

    @staticmethod
    def get_sequence_stats(sequence: str) -> Dict:
        sequence = sequence.upper()
        total_len = len(sequence)
        return {
            'length': total_len,
            'gc_content': (sequence.count('G') + sequence.count('C')) / total_len * 100 if total_len > 0 else 0,
            'n_content': sequence.count('N') / total_len * 100 if total_len > 0 else 0,
            'valid_ratio': len(set(sequence) & set('ATCGN')) / total_len * 100 if total_len > 0 else 0
        }


# =====================
#   PlantCAD2 å•åºåˆ— embedder
# =====================

class PlantCAD2GeneEmbedder:
    """
    å®˜æ–¹é£æ ¼çš„ PlantCAD2 å•åºåˆ— embedding
    - single-nucleotide token
    - MLM hidden_states[-1]
    - forward + RC èåˆ
    """

    def __init__(self, model_name='kuleshov-group/PlantCAD2-Large-l48-d1536',
                 device='cuda:0'):
        print(f"\nğŸ¤– åŠ è½½ PlantCAD2 æ¨¡å‹: {model_name}")

        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)
        self.model.to(device)
        self.model.eval()

        self.config = self.model.config
        self.embedding_dim = getattr(self.config, "hidden_size", 1536)
        self.bidirectional_strategy = getattr(self.config, "bidirectional_strategy", "add")

        print(f"ğŸ“ Embedding dim = {self.embedding_dim}")
        print(f"ğŸ”„ Bidirectional strategy = {self.bidirectional_strategy}")

    def reverse_complement(self, seq: str) -> str:
        table = str.maketrans("ATCGN", "TAGCN")
        return seq.upper().translate(table)[::-1]

    def _pool(self, x: torch.Tensor, strategy="mean"):
        if strategy == "mean":
            return x.mean(dim=1).squeeze(0)
        if strategy == "max":
            return x.max(dim=1).values.squeeze(0)
        if strategy == "first":
            return x[:, 0, :].squeeze(0)
        if strategy == "last":
            return x[:, -1, :].squeeze(0)
        return x.mean(dim=1).squeeze(0)

    def get_single_embedding(self, sequence: str, pooling_strategy: str = 'mean') -> np.ndarray:
        sequence = sequence.upper()
        if len(sequence) > 8192:
            sequence = sequence[:8192]
        elif len(sequence) < 50:
            print(f"âš ï¸  åºåˆ—è¿‡çŸ­ ({len(sequence)} bp)")

        # forward
        inputs = self.tokenizer(
            sequence,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=8192
        )
        input_ids = inputs["input_ids"].to(self.device)

        with torch.no_grad():
            out_fwd = self.model(
                input_ids=input_ids,
                output_hidden_states=True,
                return_dict=True
            )
            emb_fwd = out_fwd.hidden_states[-1]

        # reverse-complement
        rc_seq = self.reverse_complement(sequence)
        rc_inputs = self.tokenizer(
            rc_seq,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=8192
        )
        rc_ids = rc_inputs["input_ids"].to(self.device)

        with torch.no_grad():
            out_rc = self.model(
                input_ids=rc_ids,
                output_hidden_states=True,
                return_dict=True
            )
            emb_rc = out_rc.hidden_states[-1]
            emb_rc = torch.flip(emb_rc, [1])

        if self.bidirectional_strategy == "add":
            emb = (emb_fwd + emb_rc) / 2
        elif self.bidirectional_strategy == "ew_multiply":
            emb = emb_fwd * emb_rc
        else:
            emb = (emb_fwd + emb_rc) / 2

        pooled = self._pool(emb, pooling_strategy)
        return pooled.cpu().numpy()


# =====================
#   Attention Pool + Locus çº§é•¿çª—å£ embedding
# =====================

class AttentionPool(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.att = nn.Linear(dim, 1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [num_windows, dim]
        """
        scores = torch.softmax(self.att(x), dim=0)  # [num_windows, 1]
        return torch.sum(scores * x, dim=0)         # [dim]


class LocusEmbedderLongWindow:
    """
    ç»Ÿä¸€çš„ locus embeddingï¼ˆåŸºå› åŒº & åŸºå› é—´åŒºï¼‰ï¼š
    - ä»»æ„é•¿åº¦åºåˆ—
    - 8192bp window + 4096bp stride
    - Attention pooling èåˆå¤šçª—å£
    """

    def __init__(self, plantcad_embedder: PlantCAD2GeneEmbedder,
                 window_size: int = 8192,
                 stride: int = 4096,
                 pooling: str = "mean"):
        self.embedder = plantcad_embedder
        self.window_size = window_size
        self.stride = stride
        self.pooling = pooling
        self.att_pool = AttentionPool(self.embedder.embedding_dim)

    def _split_windows(self, seq: str) -> List[str]:
        seq = seq.upper()
        L = len(seq)
        ws = self.window_size
        st = self.stride

        if L <= ws:
            return [seq]

        windows = []
        for i in range(0, L - ws + 1, st):
            windows.append(seq[i:i + ws])

        if (L - ws) % st != 0:
            windows.append(seq[-ws:])

        return windows

    def embed_locus(self, seq: str) -> np.ndarray:
        windows = self._split_windows(seq)
        window_embs = []

        for w in windows:
            emb = self.embedder.get_single_embedding(w, pooling_strategy=self.pooling)
            window_embs.append(torch.tensor(emb))

        if len(window_embs) == 1:
            return window_embs[0].numpy()

        window_embs = torch.stack(window_embs)
        locus_emb = self.att_pool(window_embs).detach().numpy()
        return locus_emb


class LocusEmbedderAdaptive:
    """
    åŸºäºåºåˆ—é•¿åº¦çš„è‡ªé€‚åº” locus embedderï¼š
    - <= short_threshold: å•çª—å£ï¼ˆæ—  attention poolingï¼‰
    - > short_threshold: long-window + attention pooling
    """

    def __init__(
        self,
        plantcad_embedder: PlantCAD2GeneEmbedder,
        short_threshold: int = 4096,
        window_size: int = 4096,
        stride: int = 2048,
        pooling: str = "mean"
    ):
        self.embedder = plantcad_embedder
        self.short_threshold = short_threshold
        self.window_size = window_size
        self.stride = stride
        self.pooling = pooling
        self.att_pool = AttentionPool(self.embedder.embedding_dim)

    def _split_windows(self, seq: str):
        L = len(seq)
        ws = self.window_size
        st = self.stride

        windows = []
        for i in range(0, L - ws + 1, st):
            windows.append(seq[i:i + ws])
        if (L - ws) % st != 0:
            windows.append(seq[-ws:])
        return windows

    def embed_locus(self, seq: str) -> np.ndarray:
        L = len(seq)

        # ===== 1ï¸âƒ£ çŸ­ geneï¼šç›´æ¥ single forward =====
        if L <= self.short_threshold:
            return self.embedder.get_single_embedding(
                seq,
                pooling_strategy=self.pooling
            )

        # ===== 2ï¸âƒ£ é•¿ geneï¼šlong-window + attention =====
        windows = self._split_windows(seq)
        window_embs = []

        for w in windows:
            emb = self.embedder.get_single_embedding(
                w,
                pooling_strategy=self.pooling
            )
            window_embs.append(torch.tensor(emb))

        window_embs = torch.stack(window_embs)  # [num_windows, dim]
        locus_emb = self.att_pool(window_embs)
        return locus_emb.detach().cpu().numpy()



class SNPIndexBuilder:
    """
    ä¸ºå•ä¸ª sample æ„å»º SNP ä½ç½®ç´¢å¼•ï¼š
    chrom -> sorted positions
    """

    def __init__(self, pop_vcf, seq_builder):
        self.pop_vcf = pop_vcf
        self.seq_builder = seq_builder

    def build_for_sample(self, sample_name: str):
        snp_index = defaultdict(list)

        if self.pop_vcf is None:
            return snp_index

        if sample_name not in self.pop_vcf.sample_index:
            return snp_index

        s_idx = self.pop_vcf.sample_index[sample_name]

        for rec in self.pop_vcf.vcf:
            # åªä¿ç•™ SNP
            if len(rec.REF) != 1 or not rec.ALT or len(rec.ALT[0]) != 1:
                continue

            gts = rec.genotypes
            if s_idx >= len(gts):
                continue

            gt = gts[s_idx][:2]
            if gt[0] is None or gt[1] is None:
                continue

            if 1 not in gt:
                continue

            fasta_chrom = self.seq_builder._get_actual_chromosome(rec.CHROM)
            snp_index[fasta_chrom].append(rec.POS)

        # æ’åºï¼ˆä¸ºäºŒåˆ†æŸ¥æ‰¾ï¼‰
        for chrom in snp_index:
            snp_index[chrom].sort()

        return snp_index

    def build_for_sample_chrom(self, sample_name: str, fasta_chrom: str, chrom_len: int):
        snp_index = defaultdict(list)
        if self.pop_vcf is None:
            return snp_index

        vcf_chrom = self.seq_builder._get_vcf_chrom_from_fasta(fasta_chrom)
        region = f"{vcf_chrom}:1-{chrom_len}"

        s_idx = self.pop_vcf.sample_index.get(sample_name, None)
        if s_idx is None:
            return snp_index

        for rec in self.pop_vcf.vcf(region):   # âœ… åªè¿­ä»£è¿™ä¸€æ¡æŸ“è‰²ä½“ï¼ˆtabixï¼‰
            if len(rec.REF) != 1 or not rec.ALT or len(rec.ALT[0]) != 1:
                continue
            gt = rec.genotypes[s_idx][:2]
            if gt[0] is None or gt[1] is None:
                continue
            if 1 in gt:
                snp_index[fasta_chrom].append(rec.POS)

        snp_index[fasta_chrom].sort()
        return snp_index




# =====================
#   åºåˆ—æ„å»ºï¼ˆreference + mutantï¼‰
# =====================

class SequenceBuilder:
    """
    æ„å»ºå‚è€ƒ & æ ·æœ¬ç‰¹å¼‚åºåˆ—ï¼š
    - åŸºå› åŒºï¼šé€šè¿‡ gene_db + GFF
    - åŸºå› é—´åŒºï¼šé€šè¿‡ intergenic region åæ ‡
    - mutantï¼šä½¿ç”¨ PopulationVCF åœ¨å¯¹åº” region æ³¨å…¥ SNP
    """

    def __init__(self, genome: Dict, gene_db: GeneDatabase,
                 pop_vcf: Optional[PopulationVCF] = None,
                 chrom_mapping: Optional[Dict[str, Dict[str, str]]] = None):
        self.genome = genome
        self.gene_db = gene_db
        self.pop_vcf = pop_vcf
        self.chrom_mapping = chrom_mapping

    def _get_actual_chromosome(self, gff_chrom: str) -> str:
        """
        GFF æŸ“è‰²ä½“å â†’ FASTA æŸ“è‰²ä½“å
        """
        if self.chrom_mapping:
            for std, m in self.chrom_mapping.items():
                if m.get('gff') == gff_chrom or m.get('fasta') == gff_chrom:
                    fasta_chrom = m.get('fasta')
                    if fasta_chrom:
                        return fasta_chrom
        return gff_chrom

    def _get_vcf_chrom_from_fasta(self, fasta_chrom: str) -> str:
        """
        FASTA æŸ“è‰²ä½“å â†’ VCF æŸ“è‰²ä½“å
        """
        if self.chrom_mapping:
            for std, m in self.chrom_mapping.items():
                if m.get('fasta') == fasta_chrom:
                    vcf_chrom = m.get('vcf')
                    if vcf_chrom:
                        return vcf_chrom
        return fasta_chrom

    # ---------- å‚è€ƒåºåˆ— ----------

    def build_reference_sequence(self, gene_id: str, flank: int = 0) -> Optional[str]:
        info = self.gene_db.get_gene_info(gene_id)
        if not info:
            return None

        fasta_chrom = self._get_actual_chromosome(info['chrom'])
        if fasta_chrom not in self.genome:
            print(f"âš ï¸  æŸ“è‰²ä½“ {fasta_chrom} åœ¨ FASTA ä¸­æœªæ‰¾åˆ°")
            return None

        start = max(1, info['start'] - flank)
        end = min(len(self.genome[fasta_chrom]), info['end'] + flank)

        seq = str(self.genome[fasta_chrom].seq[start - 1:end]).upper()
        if info['strand'] == '-':
            seq = str(Seq(seq).reverse_complement())
        return seq

    # ---------- mutant é€šç”¨ locus åºåˆ— ----------

    def build_sample_locus_sequence(
        self,
        fasta_chrom: str,
        start: int,
        end: int,
        sample_name: str,
        strand: Optional[str] = None
    ) -> Optional[str]:
        """
        æ„å»ºæŸä¸ª locusï¼ˆåŸºå›  or é—´åŒºï¼‰åœ¨æŸä¸ªæ ·æœ¬ä¸‹çš„ mutant åºåˆ—ï¼ˆåªæ³¨å…¥ SNPï¼‰
        start/end: 1-based inclusive
        """
        if fasta_chrom not in self.genome:
            return None
        if self.pop_vcf is None:
            # æ²¡æœ‰ VCFï¼Œå°±é€€åŒ–ä¸º reference
            seq = str(self.genome[fasta_chrom].seq[start - 1:end]).upper()
            if strand == '-':
                seq = str(Seq(seq).reverse_complement())
            return seq

        ref_seq = str(self.genome[fasta_chrom].seq[start - 1:end]).upper()
        if len(ref_seq) == 0:
            return None

        seq_list = list(ref_seq)

        vcf_chrom = self._get_vcf_chrom_from_fasta(fasta_chrom)

        if sample_name not in self.pop_vcf.sample_index:
            return None
        s_idx = self.pop_vcf.sample_index[sample_name]

        region = f"{vcf_chrom}:{start}-{end}"
        try:
            for rec in self.pop_vcf.vcf(region):
                pos = rec.POS
                ref = rec.REF
                alt = rec.ALT[0] if rec.ALT else None

                # åªå¤„ç† SNP
                if len(ref) != 1 or (not alt or len(alt) != 1):
                    continue

                gts = rec.genotypes
                if s_idx >= len(gts):
                    continue
                gt = gts[s_idx][:2]
                if gt[0] is None or gt[1] is None:
                    continue

                # æºå¸¦ ALT ç­‰ä½åŸºå› 
                if 1 in gt:
                    rel_pos = pos - start  # 0-based ç›¸å¯¹åæ ‡
                    if 0 <= rel_pos < len(seq_list):
                        seq_list[rel_pos] = alt
        except Exception as e:
            print(f"âš ï¸  åœ¨ region {region} è¯»å– VCF æ—¶å‡ºé”™: {e}")

        sample_seq = ''.join(seq_list)
        if strand == '-':
            sample_seq = str(Seq(sample_seq).reverse_complement())
        return sample_seq

    # ---------- mutant åŸºå› åºåˆ— ----------

    def build_sample_gene_sequence(self, gene_id: str, sample_name: str, flank: int = 0) -> Optional[str]:
        info = self.gene_db.get_gene_info(gene_id)
        if not info:
            return None

        fasta_chrom = self._get_actual_chromosome(info['chrom'])
        if fasta_chrom not in self.genome:
            return None

        start = max(1, info['start'] - flank)
        end = min(len(self.genome[fasta_chrom]), info['end'] + flank)
        strand = info['strand']

        return self.build_sample_locus_sequence(
            fasta_chrom=fasta_chrom,
            start=start,
            end=end,
            sample_name=sample_name,
            strand=strand
        )

    def build_all_samples_for_gene(self, gene_id: str, flank: int = 0,
                                   sample_subset: Optional[List[str]] = None) -> Dict[str, str]:
        sequences = {}
        if self.pop_vcf is None:
            return sequences

        if sample_subset:
            target_samples = [s for s in sample_subset if s in self.pop_vcf.samples]
        else:
            target_samples = self.pop_vcf.samples

        for s in target_samples:
            seq = self.build_sample_gene_sequence(gene_id, s, flank)
            if seq:
                sequences[s] = seq
        return sequences

    def region_has_snp(
        self,
        fasta_chrom: str,
        start: int,
        end: int,
        sample_name: str
    ) -> bool:
        """
        åˆ¤æ–­æŸä¸ªæ ·æœ¬åœ¨æŒ‡å®š region æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ª SNP
        """
        if self.pop_vcf is None:
            return False

        if sample_name not in self.pop_vcf.sample_index:
            return False

        vcf_chrom = self._get_vcf_chrom_from_fasta(fasta_chrom)
        s_idx = self.pop_vcf.sample_index[sample_name]

        region = f"{vcf_chrom}:{start}-{end}"

        try:
            for rec in self.pop_vcf.vcf(region):
                # åªå…³å¿ƒ SNP
                if len(rec.REF) != 1 or not rec.ALT or len(rec.ALT[0]) != 1:
                    continue

                gts = rec.genotypes
                if s_idx >= len(gts):
                    continue

                gt = gts[s_idx][:2]
                if gt[0] is None or gt[1] is None:
                    continue

                # æ ·æœ¬æºå¸¦ ALT
                if 1 in gt:
                    return True
        except Exception:
            return False

        return False



# =====================
#   åŸºå› é—´åŒºæ„å»º
# =====================

def build_intergenic_regions(gene_db: GeneDatabase,
                             genome: Dict,
                             chrom_mapping: Optional[Dict[str, Dict[str, str]]] = None) -> List[dict]:
    print("\nğŸ§± æ„å»ºåŸºå› é—´åŒº (intergenic regions)...")

    gff_to_std = {}
    fasta_len = {}

    if chrom_mapping:
        for std, m in chrom_mapping.items():
            gff_ch = m.get('gff')
            fa_ch = m.get('fasta')
            if gff_ch:
                gff_to_std[gff_ch] = std
            if fa_ch and fa_ch in genome:
                fasta_len[std] = len(genome[fa_ch])

    chrom_to_genes = {}
    for gid, info in gene_db.genes.items():
        gff_chrom = info['chrom']
        std_chrom = gff_to_std.get(gff_chrom, gff_chrom)
        chrom_to_genes.setdefault(std_chrom, []).append((gid, info))

    intergenic_regions = []

    for std_chrom, g_list in chrom_to_genes.items():
        fasta_chrom = None
        if chrom_mapping and std_chrom in chrom_mapping:
            fasta_chrom = chrom_mapping[std_chrom].get('fasta')
        else:
            fasta_chrom = std_chrom

        if fasta_chrom not in genome:
            print(f"âš ï¸  æ ‡å‡†æŸ“è‰²ä½“ {std_chrom} å¯¹åº”çš„ FASTA æŸ“è‰²ä½“ {fasta_chrom} æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
            continue

        chrom_len = len(genome[fasta_chrom])
        g_list.sort(key=lambda x: x[1]['start'])

        first_start = g_list[0][1]['start']
        if first_start > 1:
            intergenic_regions.append({
                'chrom_std': std_chrom,
                'chrom_fasta': fasta_chrom,
                'start': 1,
                'end': first_start - 1,
                'id': f'{std_chrom}_intergenic_0000'
            })

        for i in range(len(g_list) - 1):
            cur_end = g_list[i][1]['end']
            next_start = g_list[i + 1][1]['start']
            if next_start > cur_end + 1:
                intergenic_regions.append({
                    'chrom_std': std_chrom,
                    'chrom_fasta': fasta_chrom,
                    'start': cur_end + 1,
                    'end': next_start - 1,
                    'id': f'{std_chrom}_intergenic_{i + 1:04d}'
                })

        last_end = g_list[-1][1]['end']
        if last_end < chrom_len:
            intergenic_regions.append({
                'chrom_std': std_chrom,
                'chrom_fasta': fasta_chrom,
                'start': last_end + 1,
                'end': chrom_len,
                'id': f'{std_chrom}_intergenic_tail'
            })

    print(f"âœ… å…±æ„å»º {len(intergenic_regions)} ä¸ª intergenic loci")
    return intergenic_regions


# =====================
#   å·¥å…·å‡½æ•°
# =====================

def get_directory_size(path: str) -> float:
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total_size += os.path.getsize(filepath)
    return total_size / (1024**3)


def region_has_snp_fast(
    snp_index: dict,
    chrom: str,
    start: int,
    end: int
) -> bool:
    """
    O(log N) åˆ¤æ–­ region æ˜¯å¦åŒ…å« SNP
    """
    if chrom not in snp_index:
        return False

    positions = snp_index[chrom]
    i = bisect_left(positions, start)
    return i < len(positions) and positions[i] <= end

# =====================
#   ä¸»æµç¨‹
# =====================

def main():
    parser = argparse.ArgumentParser(description='Genome-wide reference + per-sample mutant embedding with PlantCAD2')

    parser.add_argument('--fasta', required=True, help='å‚è€ƒåŸºå› ç»„ FASTA æ–‡ä»¶')
    parser.add_argument('--vcf', required=True, help='ç¾¤ä½“ VCF æ–‡ä»¶')
    parser.add_argument('--gff', required=True, help='GFF åŸºå› æ³¨é‡Šæ–‡ä»¶')
    parser.add_argument('--output', default='./genome_embeddings', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model', default='kuleshov-group/PlantCAD2-Large-l48-d1536')
    parser.add_argument('--device', default='cuda:0')
    parser.add_argument('--flank', type=int, default=0, help='åŸºå› ä¸¤ä¾§å»¶ä¼¸ç¢±åŸºæ•°')
    parser.add_argument('--gene_list', type=str,
                        help='åŒ…å« gene ID çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼›è‹¥æä¾›ï¼Œä»…å¤„ç†å…¶ä¸­çš„åŸºå› ')
    parser.add_argument('--max_genes', type=int, help='æœ€å¤§å¤„ç†åŸºå› æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--samples', nargs='*',
                        help='å¯é€‰ï¼šåªå¤„ç†æŒ‡å®šæ ·æœ¬åï¼›é»˜è®¤ VCF ä¸­å…¨éƒ¨æ ·æœ¬')

    parser.add_argument(
        '--mode',
        choices=['reference', 'sample', 'all'],
        default='all',
        help='run mode: '
            'reference=only reference, '
            'sample=only sample (requires existing reference), '
            'all=reference+sample (not recommended for multi-GPU)'
    )

    parser.add_argument(
        '--chrom',
        type=str,
        default=None,
        help='åªå¤„ç†æŸæ¡ FASTA æŸ“è‰²ä½“ï¼ˆä¾‹å¦‚ Chr1ï¼‰ï¼›ä¸æŒ‡å®šåˆ™å…¨åŸºå› ç»„'
    )

    args = parser.parse_args()
    selected_chrom = args.chrom  # e.g. "Chr1" in FASTA naming


    print("=" * 80)
    print("ğŸ§¬ å…¨åŸºå› ç»„ PlantCAD2 Embedding ç³»ç»Ÿ")
    print("=" * 80)
    print(f"ğŸ“ FASTA: {args.fasta}")
    print(f"ğŸ“Š VCF:   {args.vcf}")
    print(f"ğŸ“‹ GFF:   {args.gff}")
    print("=" * 80)

    output_dir = ensure_dir(args.output)
    ensure_dir(output_dir / "embeddings")
    ensure_dir(output_dir / "logs")
    ensure_dir(output_dir / "checkpoints")
    ensure_dir(output_dir / "sequences")

    #ref_gene_pkl = output_dir / "ref_gene_embeddings.pkl"
    #ref_intergenic_pkl = output_dir / "intergenic_embeddings.pkl"


    # 0. è¾“å…¥éªŒè¯
    if not DataValidator.validate_inputs(args.fasta, args.vcf, args.gff):
        print("âŒ è¾“å…¥æ–‡ä»¶éªŒè¯å¤±è´¥")
        return

    # ç¡®ä¿ VCF bgzip + indexï¼Œå¹¶ä½¿ç”¨è¯¥è·¯å¾„
    vcf_path = DataValidator.ensure_bgzip_and_index(args.vcf)

    # 1. åŠ è½½ FASTA
    print("\nğŸ“¥ æ­¥éª¤1: åŠ è½½ FASTA åºåˆ—")
    print("-" * 40)
    genome = SeqIO.to_dict(SeqIO.parse(args.fasta, "fasta"))
    genome_keys = list(genome.keys())
    print(f"âœ… åŠ è½½ {len(genome)} æ¡æŸ“è‰²ä½“")

    # 2. åŠ è½½ GFF
    print("\nğŸ“¥ æ­¥éª¤2: åŠ è½½ GFF æ³¨é‡Š")
    print("-" * 40)
    gene_db = GeneDatabase(args.gff)
    all_genes = list(gene_db.genes.keys())
    print(f"ğŸ”¹ GFF ä¸­åŸºå› æ•°é‡: {len(all_genes)}")

    # 3. åŠ è½½ VCF & æŸ“è‰²ä½“æ˜ å°„
    print("\nğŸ“¥ æ­¥éª¤3: åŠ è½½ VCF & æŸ“è‰²ä½“å‘½åæ ‡å‡†åŒ–")
    print("-" * 40)
    pop_vcf = PopulationVCF(vcf_path)
    vcf_chroms = pop_vcf.get_chroms()
    gff_chroms = list({g['chrom'] for g in gene_db.genes.values()})

    chrom_mapping = DataValidator.normalize_chromosome_names(
        genome_keys, vcf_chroms, gff_chroms
    )

    # 4. åŸºå› åˆ—è¡¨
    print("\nğŸ” æ­¥éª¤4: å‡†å¤‡åŸºå› åˆ—è¡¨")
    print("-" * 40)

    if args.gene_list:
        print(f"ğŸ“„ ä½¿ç”¨ gene_list æ–‡ä»¶: {args.gene_list}")
        with open(args.gene_list) as f:
            filtered_genes = [line.strip() for line in f if line.strip() in gene_db.genes]
        print(f"ğŸ“Š æ¥è‡ª gene_list çš„åŸºå› æ•°é‡: {len(filtered_genes)}")
    else:
        filtered_genes = list(gene_db.genes.keys())
        if args.max_genes:
            filtered_genes = filtered_genes[:args.max_genes]
            print(f"âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {args.max_genes} ä¸ªåŸºå› ")

    print(f"ğŸ“Š æœ€ç»ˆå¤„ç†åŸºå› æ•°: {len(filtered_genes)}")

    # 5. åˆå§‹åŒ– PlantCAD2
    print("\nğŸ¤– æ­¥éª¤5: åˆå§‹åŒ– PlantCAD2 æ¨¡å‹")
    print("-" * 40)
    plantcad_embedder = PlantCAD2GeneEmbedder(
        model_name=args.model,
        device=args.device
    )
    '''
    locus_embedder = LocusEmbedderLongWindow(
        plantcad_embedder=plantcad_embedder,
        window_size=8192,
        stride=4096,
        pooling="mean"
    )
    '''
    locus_embedder = LocusEmbedderAdaptive(
        plantcad_embedder=plantcad_embedder,
        short_threshold=4096,
        window_size=4096,
        stride=2048,
        pooling="mean"
    )
    print("âœ… PlantCAD2 æ¨¡å‹å·²åŠ è½½")
    # 6. åˆå§‹åŒ– SequenceBuilder
    print("\nâš™ï¸ æ­¥éª¤6: åˆå§‹åŒ– SequenceBuilder")
    print("-" * 40)
    seq_builder = SequenceBuilder(genome, gene_db, pop_vcf, chrom_mapping)
    print("âœ… åºåˆ—æ„å»ºå™¨å·²å‡†å¤‡")

    # 7. æ„å»º intergenic åŒº
    print("\nğŸ§± æ­¥éª¤7: æ„å»º intergenic regions")
    print("-" * 40)
    intergenic_regions = build_intergenic_regions(gene_db, genome, chrom_mapping)

    suffix = f".{selected_chrom}" if selected_chrom else ""
    ref_gene_pkl = output_dir / f"ref_gene_embeddings{suffix}.pkl"
    ref_intergenic_pkl = output_dir / f"intergenic_embeddings{suffix}.pkl"

    if selected_chrom:
        # 1) è¿‡æ»¤ genes
        filtered_genes = [
            gid for gid in filtered_genes
            if seq_builder._get_actual_chromosome(gene_db.get_gene_info(gid)['chrom']) == selected_chrom
        ]

        # 2) è¿‡æ»¤ intergenic
        intergenic_regions = [
            r for r in intergenic_regions
            if r['chrom_fasta'] == selected_chrom
        ]

        print(f"ğŸ§© Chromosome mode = {selected_chrom}: genes={len(filtered_genes)}, intergenic={len(intergenic_regions)}")

    # 8. ç”Ÿæˆå‚è€ƒåŸºå›  + å‚è€ƒé—´åŒº embedding
    print("\nğŸš€ æ­¥éª¤8: ç”Ÿæˆ reference gene & intergenic embedding")
    print("-" * 40)
    '''
    ref_gene_embeddings = {}
    failed_genes = []

    for gid in tqdm(filtered_genes, desc="Embedding reference genes"):
        try:
            seq = seq_builder.build_reference_sequence(gid, flank=args.flank)
            if not seq:
                failed_genes.append((gid, "no_sequence"))
                continue

            clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
            stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
            if stats['length'] < 50 or stats['n_content'] > 50:
                failed_genes.append((gid, "low_quality"))
                continue

            emb = locus_embedder.embed_locus(clean_seq)
            ref_gene_embeddings[gid] = emb
        except Exception as e:
            print(f"âŒ åŸºå›  {gid} å¤„ç†å¤±è´¥: {e}")
            failed_genes.append((gid, str(e)))
            continue

    ref_pkl = output_dir / "ref_gene_embeddings.pkl"
    with open(ref_pkl, "wb") as f:
        pickle.dump(ref_gene_embeddings, f)
    print(f"ğŸ’¾ å‚è€ƒåŸºå›  embedding å·²ä¿å­˜åˆ°: {ref_pkl}")
    print(f"âœ… æˆåŠŸåŸºå› : {len(ref_gene_embeddings)}, å¤±è´¥: {len(failed_genes)}")

    # å‚è€ƒ intergenic
    intergenic_embeddings = {}
    for reg in tqdm(intergenic_regions, desc="Embedding reference intergenic"):
        chrom_fa = reg['chrom_fasta']
        start = reg['start']
        end = reg['end']

        try:
            seq = str(genome[chrom_fa].seq[start - 1:end]).upper()
            clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
            stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
            if stats['length'] < 50 or stats['n_content'] > 50:
                continue

            emb = locus_embedder.embed_locus(clean_seq)
            intergenic_embeddings[reg['id']] = {
                'chrom': chrom_fa,
                'start': start,
                'end': end,
                'embedding': emb
            }
        except Exception as e:
            print(f"âŒ é—´åŒº {reg['id']} å¤„ç†å¤±è´¥: {e}")
            continue

    intergenic_pkl = output_dir / "intergenic_embeddings.pkl"
    with open(intergenic_pkl, "wb") as f:
        pickle.dump(intergenic_embeddings, f)
    print(f"ğŸ’¾ å‚è€ƒ intergenic embedding å·²ä¿å­˜åˆ°: {intergenic_pkl}")
    print(f"âœ… é—´åŒºæ•°é‡: {len(intergenic_embeddings)}")
    '''

    # ===============================
    # Step 8: Reference embedding
    # ===============================
    ref_gene_embeddings = {}
    intergenic_embeddings = {}

    if args.mode in ['reference', 'all']:
        print("\nğŸš€ Step 8: ç”Ÿæˆ reference gene & intergenic embedding")

        # ---- reference gene ----
        failed_genes = []
        for gid in tqdm(filtered_genes, desc="Embedding reference genes"):
            try:
                seq = seq_builder.build_reference_sequence(gid, flank=args.flank)
                if not seq:
                    continue

                clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
                stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
                if stats['length'] < 50 or stats['n_content'] > 50:
                    continue

                emb = locus_embedder.embed_locus(clean_seq)
                ref_gene_embeddings[gid] = emb

            except Exception as e:
                failed_genes.append((gid, str(e)))

        with open(ref_gene_pkl, "wb") as f:
            pickle.dump(ref_gene_embeddings, f)

        print(f"ğŸ’¾ reference gene embedding saved: {ref_gene_pkl}")

        # ---- reference intergenic ----
        for reg in tqdm(intergenic_regions, desc="Embedding reference intergenic"):
            chrom_fa = reg['chrom_fasta']
            start = reg['start']
            end = reg['end']

            try:
                seq = str(genome[chrom_fa].seq[start - 1:end]).upper()
                clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
                stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
                if stats['length'] < 50 or stats['n_content'] > 50:
                    continue

                emb = locus_embedder.embed_locus(clean_seq)
                intergenic_embeddings[reg['id']] = {
                    'chrom': chrom_fa,
                    'start': start,
                    'end': end,
                    'embedding': emb
                }

            except Exception:
                continue

        with open(ref_intergenic_pkl, "wb") as f:
            pickle.dump(intergenic_embeddings, f)

        print(f"ğŸ’¾ reference intergenic embedding saved: {ref_intergenic_pkl}")


    # ===============================
    # Step 8 å®Œæˆå
    # ===============================
    if args.mode == 'reference':
        print("\nğŸ‰ Reference embedding å®Œæˆï¼Œç¨‹åºé€€å‡ºï¼ˆmode=referenceï¼‰")
        return

    # ===============================
    # Step 9: Sample embedding
    # ===============================
    if args.mode in ['sample', 'all']:

        if not ref_gene_pkl.exists() or not ref_intergenic_pkl.exists():
            raise RuntimeError(
                "âŒ reference embedding ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ --mode reference"
            )

        print("\nğŸ“¥ åŠ è½½ reference embeddings ...")
        with open(ref_gene_pkl, "rb") as f:
            ref_gene_embeddings = pickle.load(f)

        with open(ref_intergenic_pkl, "rb") as f:
            intergenic_embeddings = pickle.load(f)

        print(
            f"âœ… reference loaded: "
            f"{len(ref_gene_embeddings)} genes, "
            f"{len(intergenic_embeddings)} intergenic"
        )

    ##############################################
    ##############################################
    ##############################################
    
    # 9. ç”Ÿæˆæ¯ä¸ªæ ·æœ¬çš„ mutant embeddingï¼ˆåŸºå›  + é—´åŒºï¼‰
    print("\nğŸš€ æ­¥éª¤9: ç”Ÿæˆæ¯ä¸ªæ ·æœ¬çš„ mutant embeddingï¼ˆgene + intergenic, SNP onlyï¼‰")
    print("ğŸ”’ reference embedding ä»¥åªè¯»æ¨¡å¼åŠ è½½ï¼ˆsample-onlyï¼‰")
    print("-" * 40)

    # æ ·æœ¬åˆ—è¡¨
    if args.samples:
        target_samples = [s for s in pop_vcf.samples if s in args.samples]
        print(f"ğŸ‘¥ ä½¿ç”¨æ ·æœ¬å­é›†: {len(target_samples)}/{len(pop_vcf.samples)}")
    else:
        target_samples = pop_vcf.samples
        print(f"ğŸ‘¥ å…¨éƒ¨æ ·æœ¬æ•°: {len(target_samples)}")

    for sample in target_samples:
        #sample_file = output_dir / f"sample_{sample}_embeddings.pkl"
        sample_file = output_dir / f"sample_{sample}{suffix}_embeddings.pkl"

        if sample_file.exists():
            print(f"â­ï¸  æ ·æœ¬ {sample} å·²å­˜åœ¨ç»“æœï¼Œè·³è¿‡ï¼ˆå¯ä½œä¸º resumeï¼‰")
            continue

        print(f"\nğŸ§¬ æ ·æœ¬ {sample}: ç”Ÿæˆ mutant embedding ...")
        
        snp_gene_count = 0
        snp_intergenic_count = 0

        t0 = time.perf_counter()

        print(f"ğŸ§  æ„å»º SNP indexï¼ˆæ ·æœ¬ {sample}ï¼‰...")
        #snp_index = SNPIndexBuilder(pop_vcf, seq_builder).build_for_sample(sample)
        
        chrom_len = len(genome[selected_chrom])

        snp_index = SNPIndexBuilder(
            pop_vcf, seq_builder
        ).build_for_sample_chrom(
            sample_name=sample,
            fasta_chrom=selected_chrom,
            chrom_len=chrom_len
        )

        print(f"   SNP æ€»æ•°: {sum(len(v) for v in snp_index.values())}")


        sample_gene_embs = {}
        sample_intergenic_embs = {}

        '''
        # åŸºå› 
        for gid in tqdm(filtered_genes, desc=f"{sample} genes", leave=False):
            try:
                seq = seq_builder.build_sample_gene_sequence(gid, sample, flank=args.flank)
                if not seq:
                    continue
                clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
                stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
                if stats['length'] < 50 or stats['n_content'] > 50:
                    continue
                emb = locus_embedder.embed_locus(clean_seq)
                sample_gene_embs[gid] = emb
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample}, åŸºå›  {gid} å¤±è´¥: {e}")
                continue
        '''

        # åŸºå› 
        for gid in tqdm(filtered_genes, desc=f"{sample} genes", leave=False):
            try:
                info = gene_db.get_gene_info(gid)
                if not info:
                    continue

                fasta_chrom = seq_builder._get_actual_chromosome(info['chrom'])
                start = max(1, info['start'] - args.flank)
                end = min(len(genome[fasta_chrom]), info['end'] + args.flank)

                # ===== SNP-aware åˆ¤æ–­ =====
                has_snp = region_has_snp_fast(
                snp_index=snp_index,
                chrom=fasta_chrom,
                start=start,
                end=end)

                if has_snp:
                    snp_gene_count += 1

                # ===== æ²¡æœ‰ SNPï¼šç›´æ¥å¤ç”¨ reference =====
                if not has_snp:
                    if gid in ref_gene_embeddings:
                        sample_gene_embs[gid] = ref_gene_embeddings[gid]
                    continue   # â†â†â† å…³é”®ï¼ç›´æ¥è·³åˆ°ä¸‹ä¸€ä¸ª gene

                # ===== æœ‰ SNPï¼šæ‰çœŸæ­£æ„å»º mutant + embed =====
                seq = seq_builder.build_sample_gene_sequence(gid, sample, flank=args.flank)
                if not seq:
                    continue

                clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
                stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
                if stats['length'] < 50 or stats['n_content'] > 50:
                    continue

                emb = locus_embedder.embed_locus(clean_seq)
                sample_gene_embs[gid] = emb

            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample}, åŸºå›  {gid} å¤±è´¥: {e}")
                continue

        '''                    
        # é—´åŒº
        for reg in tqdm(intergenic_regions, desc=f"{sample} intergenic", leave=False):
            chrom_fa = reg['chrom_fasta']
            start = reg['start']
            end = reg['end']
            rid = reg['id']

            try:
                seq = seq_builder.build_sample_locus_sequence(
                    fasta_chrom=chrom_fa,
                    start=start,
                    end=end,
                    sample_name=sample,
                    strand=None
                )
                if not seq:
                    continue
                clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
                stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
                if stats['length'] < 50 or stats['n_content'] > 50:
                    continue

                emb = locus_embedder.embed_locus(clean_seq)
                sample_intergenic_embs[rid] = {
                    'chrom': chrom_fa,
                    'start': start,
                    'end': end,
                    'embedding': emb
                }
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample}, é—´åŒº {rid} å¤±è´¥: {e}")
                continue
        '''

        # é—´åŒº
        for reg in tqdm(intergenic_regions, desc=f"{sample} intergenic", leave=False):
            chrom_fa = reg['chrom_fasta']
            start = reg['start']
            end = reg['end']
            rid = reg['id']

            try:
                # ===== SNP-aware åˆ¤æ–­ =====
                has_snp = region_has_snp_fast(
                snp_index=snp_index,
                chrom=chrom_fa,
                start=start,
                end=end
            )

                # ===== æ²¡æœ‰ SNPï¼šç›´æ¥å¤ç”¨ reference intergenic =====
                if has_snp:
                    snp_intergenic_count += 1
                else:
                    # æ²¡æœ‰ SNPï¼šç›´æ¥å¤ç”¨ reference
                    if rid in intergenic_embeddings:
                        sample_intergenic_embs[rid] = intergenic_embeddings[rid]
                    continue   # â† ä¸ç®¡æœ‰æ²¡æœ‰ referenceï¼Œéƒ½ä¸å†ç®— mutant

                # ===== æœ‰ SNPï¼šæ‰é‡æ–°æ„å»º + embed =====
                seq = seq_builder.build_sample_locus_sequence(
                    fasta_chrom=chrom_fa,
                    start=start,
                    end=end,
                    sample_name=sample,
                    strand=None
                )
                if not seq:
                    continue

                clean_seq, _ = SequenceQualityChecker.validate_dna_sequence(seq)
                stats = SequenceQualityChecker.get_sequence_stats(clean_seq)
                if stats['length'] < 50 or stats['n_content'] > 50:
                    continue
                emb = locus_embedder.embed_locus(clean_seq)
                sample_intergenic_embs[rid] = {
                    'chrom': chrom_fa,
                    'start': start,
                    'end': end,
                    'embedding': emb
                }

            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample}, é—´åŒº {rid} å¤±è´¥: {e}")
                continue

        sample_data = {
            'sample': sample,
            'gene_embeddings': sample_gene_embs,
            'intergenic_embeddings': sample_intergenic_embs
        }

        with open(sample_file, "wb") as f:
            pickle.dump(sample_data, f)

        t1 = time.perf_counter()
        elapsed_min = (t1 - t0) / 60

        print(f"ğŸ’¾ æ ·æœ¬ {sample} çš„ mutant embedding å·²ä¿å­˜åˆ°: {sample_file}")
        print(f"   åŸºå› æ•°: {len(sample_gene_embs)}, é—´åŒºæ•°: {len(sample_intergenic_embs)}")
        print(f"â±ï¸  æ ·æœ¬ {sample} ç”¨æ—¶: {elapsed_min:.2f} åˆ†é’Ÿ")


        total_genes = len(filtered_genes)
        total_intergenic = len(intergenic_regions)

        print(f"ğŸ“Š SNP-aware ç»Ÿè®¡ï¼ˆæ ·æœ¬ {sample}ï¼‰:")
        print(f"  SNP genes:        {snp_gene_count} / {total_genes} "
            f"({snp_gene_count / total_genes * 100:.2f}%)")
        print(f"  SNP intergenic:   {snp_intergenic_count} / {total_intergenic} "
            f"({snp_intergenic_count / total_intergenic * 100:.2f}%)")
        print(f"  â±ï¸ mutant embedding ç”¨æ—¶: {elapsed_min:.2f} åˆ†é’Ÿ")


        gc.collect()

    print("\n" + "=" * 80)
    print("ğŸ‰ å…¨æµç¨‹å®Œæˆ!")
    print("=" * 80)
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
    print(f"ğŸ“ æ€»æ–‡ä»¶å¤§å°: {get_directory_size(args.output):.2f} GB")


if __name__ == "__main__":
    main()
